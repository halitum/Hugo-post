+++
title = "[Datawhale AI å¤ä»¤è¥] Task 3 æºå¤§æ¨¡å‹RAGå®æˆ˜"
date = "2024-08-17"
description = ""
tags = [
 "LLM",
]
categories = [
 "DataWhaleç»„é˜Ÿå­¦ä¹ ",
]
image = "https://typora-picturelib.oss-cn-beijing.aliyuncs.com/1-1-0.png"

+++
## # RAGç®€ä»‹

### ç”¨é€”ï¼š

è§£å†³äº†å¤§å‹æ¨¡å‹åœ¨çŸ¥è¯†å±€é™æ€§ã€æ•°æ®å®‰å…¨æ€§å’Œç”Ÿæˆå¹»è§‰ç­‰æ–¹é¢çš„é—®é¢˜

<img src="https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240817234227874.png" alt="image-20240817234227874" style="zoom:67%;" />

### å…·ä½“æµç¨‹ï¼š

- ç´¢å¼•ï¼šå°†æ–‡æ¡£åº“åˆ†å‰²æˆè¾ƒçŸ­çš„ **Chunk**ï¼Œå³æ–‡æœ¬å—æˆ–æ–‡æ¡£ç‰‡æ®µï¼Œç„¶åæ„å»ºæˆå‘é‡ç´¢å¼•ã€‚

- æ£€ç´¢ï¼šè®¡ç®—é—®é¢˜å’Œ Chunks çš„ç›¸ä¼¼åº¦ï¼Œæ£€ç´¢å‡ºè‹¥å¹²ä¸ªç›¸å…³çš„ Chunkã€‚

- ç”Ÿæˆï¼šå°†æ£€ç´¢åˆ°çš„Chunksä½œä¸ºèƒŒæ™¯ä¿¡æ¯ï¼Œç”Ÿæˆé—®é¢˜çš„å›ç­”ã€‚

<img src="https://typora-picturelib.oss-cn-beijing.aliyuncs.com/1-1-0.png" alt="1-1-0"  />

------

## # æ­å»ºå¹¶ä½¿ç”¨å‘é‡æ•°æ®åº“Chroma

å®‰è£…Chroma

```bash
pip install chromadb
```

ä¸Šæ‰‹Chrome

```python
# åˆ›å»ºChromaå®¢æˆ·ç«¯
import chromadb
client = chromadb.Client()

# åˆ›å»ºé›†åˆï¼ˆé›†åˆç±»ä¼¼äºä¼ ç»Ÿæ•°æ®åº“ä¸­çš„è¡¨ï¼‰
collection = client.create_collection(name="your_collection_name")

# æ·»åŠ æ–‡æ¡£ï¼ˆChromaä¼šè‡ªåŠ¨å¤„ç†æ–‡æœ¬åµŒå…¥å’Œç´¢å¼•ï¼‰
collection.add(documents=["Your first document text"])

# æŸ¥è¯¢é›†åˆ
results = collection.query(query_texts=["Sample query"])
```

------

## # åŸºäºLangChainçš„RAGæµç¨‹

å®‰è£…LangChain

```bash
pip install langchain langchain-community langchain-core langchain-openai unstructured sentence-transformers chromadb
```

ä¸Šæ‰‹LangChain-RAGï¼ˆè¯¦è§LangChainå®˜æ–¹æ–‡æ¡£[Yuan2.0 | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.2/docs/integrations/llms/yuan2/)ï¼‰
```python
# åˆ›å»ºLLMå®¢æˆ·ç«¯ï¼ˆåŸºäºYuan2.0 Inference-Serverï¼‰
from langchain.chains import LLMChain
from langchain_community.llms.yuan2 import Yuan2
infer_api = "http://127.0.0.1:8000/yuan"	# default infer_api for a local deployed Yuan2.0 inference server

# åˆå§‹åŒ–æ¨¡å‹
yuan_llm = Yuan2(
    infer_api=infer_api,
    max_tokens=2048,
    temp=1.0,
    top_p=0.9,
    use_history=False,
)

# æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²ï¼šåŠ è½½ä½ çš„æ–‡æ¡£å¹¶å°†å…¶åˆ†å‰²æˆå¯ç®¡ç†çš„å°å—
from langchain_community.document_loaders import DirectoryLoader
loader = DirectoryLoader('./documents')
documents = loader.load()
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
split_docs = text_splitter.split_documents(documents)

# åˆ›å»ºåµŒå…¥ï¼šå°†ä½ çš„æ–‡æœ¬å—è½¬æ¢æˆåµŒå…¥
from langchain.embeddings import SentenceTransformerEmbeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# åŠ è½½æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“ï¼šä½¿ç”¨Chromaå­˜å‚¨å’Œæ£€ç´¢åµŒå…¥ï¼š
from langchain.vectorstores.chroma import Chroma
db = Chroma.from_documents(documents, embeddings)

# æ£€ç´¢ç›¸å…³å†…å®¹
matching_docs = db.similarity_search('ä½ çš„æŸ¥è¯¢')

# ä»LLMç”Ÿæˆå“åº”ï¼šä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œç”¨æˆ·çš„æŸ¥è¯¢æ¥ç”Ÿæˆå“åº”
from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(llm, chain_type="stuff", verbose=True)
answer = chain.run(input_documents=matching_docs, question='ä½ çš„æŸ¥è¯¢')
```

