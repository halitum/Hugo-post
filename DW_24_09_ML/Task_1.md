+++
title = "[Datawhale吃瓜教程] Task 1"
date = "2024-09-18"
description = "概览西瓜书+南瓜书第1、2章"
tags = [
 "机器学习",
]
categories = [
 "DataWhale组队学习",
]
image = "https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240919004621136.png"

+++

# # 机器学习绪论

> *一些基本概念*
> 机器学习：研究关于“学习算法”的学科
> 模型：从数据中学习得到的结果
> 特征向量/示例：数据集中的每条记录
> 特征(feature)/属性(attribute)：反应事件或对象在模仿门面的表现或性质的事项（属性的取值即为属性值，张成的空间为属性空间/样本空间）

## # 什么是假设空间？

假设空间是机器学习中用于描述所有**可能假设**的集合，我们试图在这个假设空间中寻找一个或多个与训练数据一致的假设，以便能够对未知数据进行准确的预测。

![image-20240919002408070](https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240919002408070.png)

假设空间中与训练集一致的“假设集合”，称为**“版本空间”**。版本空间表示了在现有数据下，所有可能正确的模型集合。*学习的目标是缩小版本空间，理想情况下找到唯一的假设。*



## # 什么是归纳偏好(Inductive Bias)？

归纳偏好是学习过程中，对某种类型的假设所表现出的倾向，决定了机器学习算法选择哪个假设来构建模型（在训练数据不足以唯一确定模型时，归纳偏好帮助算法选择一个看起来最“正确”的假设。）

归纳偏好让算法在等效的假设中做出决策，选择出一个最适合的假设。所谓最适合的假设，就是最符合我们设定的原则的假设

> 常见的**归纳偏好原则**
>
> - 奥卡姆剃刀原则：若有多个假设与观察一致，则选最简单的那个
> - 平滑性假设：认为相似的输入应有相似的输出
> - 最大似然原则：选择使观测数据概率最大的假设

<img src="https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240919003325192.png" alt="image-20240919003325192" style="zoom:50%;" />

在这个例子中，如果A、B曲线都是符合观察的版本空间，那么如果遵循奥卡姆剃刀原则作为归纳偏好原则，那我们最终将决定使用曲线A的假设空间来构建模型



## # “**没有免费的午餐**”定理（ **NFL** 定理）

内容：当所有“问题”出现的机会相同、或所有问题同等重要时，对于任意两个学习算法而言，他们的期望性能都相同。（但是，所有问题的先验概率是均匀的，这在现实中并不成立。而正相反，数据会具有特定的分布，算法可以利用这些分布特性取得更好的性能）

结论：谈论算法的相对优劣，必须要针对具体的学习问题



## # 机器学习应用领域

- **图像识别**：用于人脸识别、手写数字识别、自动驾驶中的物体检测等。
- **自然语言处理**：包括机器翻译、情感分析、文本分类、语音识别和生成等。
- **推荐系统**：在电商平台推荐商品，在音乐和视频平台推荐内容，提高用户体验。
- **金融预测**：用于股票价格预测、信用评分、欺诈检测和风险管理。



# # 模型评估与选择

on_going…
