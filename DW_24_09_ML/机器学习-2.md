+++
title = "[Datawhale机器学习教程] 2. 线性模型-线性回归"
date = "2024-09-21"
description = "详读西瓜书+南瓜书第三章——线性回归"
tags = [
 "机器学习",
]
categories = [
 "DataWhale组队学习",
]
image = "https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240919004621136.png"

+++



**线性模型基本形式**
$$
f(x) = w^T x + b
$$

其中，$w = (w_1; w_2; \dots; w_d)$。$w$ 和 $b$ 学得之后，模型就得以确定。

线性模型形式简单易于建模，且有很好的可解释性(comprehensibility)



# 线性回归

## # 一元线性回归损失函数推导方法

### 最小二乘法

基于均方误差最小化来进行模型求解的方法称为“最小二乘法”(least square method)

<img src="https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240921231922658.png" alt="image-20240921231922658" style="zoom: 67%;" />
$$
(w^*, b^*) = \arg\min_{(w,b)} \sum_{i=1}^{m} \left( f(x_i) - y_i \right)^2
$$

$$
= \arg\min_{(w,b)} \sum_{i=1}^{m} \left( y_i - w x_i - b \right)^2
$$

> 扩展：如何将离散特征添加到模型中——
>
> <img src="https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240921233349910.png" alt="image-20240921233349910" style="zoom:50%;" />

### 极大似然估计

将线性回归模型表述为如下形式：
$$
y = wx + b + \epsilon
$$
其中 $y$ 是预测值， $wx$ 是真实值。 $\epsilon$ 是误差，且符合正态分布

+++

 $\epsilon$ 的概率密度
$$
p(\epsilon) = \frac{1}{\sqrt{2\pi\sigma}} \exp\left( -\frac{\epsilon^2}{2\sigma^2} \right)
$$

将 $\epsilon = y - (wx + b)$ 带入，得到 $y$ 的概率密度函数


$$
p(y) = \frac{1}{\sqrt{2\pi\sigma}} \exp\left( -\frac{(y - (wx + b))^2}{2\sigma^2} \right)
$$
使用极大似然估计来估计得到 $w$ 和 $b$ 的值
$$
\ln L(w, b) = m \ln \frac{1}{\sqrt{2\pi\sigma}} - \frac{1}{2\sigma^2} \sum_{i=1}^{m} (y_i - wx_i - b)^2
$$
也即
$$
(w^*, b^*) = \arg\min_{(w, b)} \sum_{i=1}^{m} (y_i - wx_i - b)^2
$$

+++

该结论与最小二乘法**得出的结论相同**——均方误差最小化可达到线性回归



## #如何求的 $w$ 和 $b$ ？

可以通过 `凸函数的判定定理`

<img src="https://typora-picturelib.oss-cn-beijing.aliyuncs.com/image-20240922003325964.png" alt="image-20240922003325964" style="zoom:50%;" />

证得线性回归损失函数是关于 $w$ 和 $b$ 的凸函数，则问题化简成求凸函数最值

由凸函数性质（凸充分定理）可知，损失函数梯度为0的点（即各个偏导数为0的点）即为损失函数的极小值点，这也是损失函数取得最小值的充分条件

带入偏导数为0，求得表达式：
$$
w = \frac{\sum_{i=1}^{m} y_i (x_i - \bar{x})}{\sum_{i=1}^{m} x_i^2 - \frac{1}{m} \left( \sum_{i=1}^{m} x_i \right)^2}
$$

$$
b = \frac{1}{m} \sum_{i=1}^{m} (y_i - wx_i)
$$



## # 表达式向量化

on going